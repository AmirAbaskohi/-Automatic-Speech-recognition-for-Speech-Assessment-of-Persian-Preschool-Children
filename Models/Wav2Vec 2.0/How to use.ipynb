{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wav2Vec with RFP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Specify you data path\n",
        "\n",
        "Here you should specify your data path as well as the directory needed for result of RFP.\n",
        "\n",
        "`DATA_DIR` should be addressed to you `.wav` files.\n",
        "\n",
        "You also need to have access to `Google Cloud` and `TPU` to run the following code. Remeber to move the `vocab files` needed to your bucket in the addresses mentioned in the code."
      ],
      "metadata": {
        "id": "1uP4ORaE8Enl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"...\"\n",
        "RESULT_DIR = \"...\""
      ],
      "metadata": {
        "id": "gdaeiHhj7LRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning the repository"
      ],
      "metadata": {
        "id": "gR_1Y9Jt6EtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/AmirAbaskohi/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children.git"
      ],
      "metadata": {
        "id": "xoDs09-46Dzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparing with RFP"
      ],
      "metadata": {
        "id": "lNFQebdo5ykV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the following cell, you can prepare data for training using RFP. Remember that you have to specify `DATA_DIR` and `RESULT_DIR` before running the cell below."
      ],
      "metadata": {
        "id": "pX5y1Gw4AWpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children/Pretrain\n",
        "\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 pitch_changer.py $DATA_DIR $RESULT_DIR\n",
        "\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "qBbHnlgb5yEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrain model"
      ],
      "metadata": {
        "id": "KV_AsHiV5s_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First specify your TPU and your BUCKET that data exists. Then use the next cell to train your data."
      ],
      "metadata": {
        "id": "c8Mxz5kfAp4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TPU_NAME = \"...\"\n",
        "BUCKET_NAME = \"...\"\n",
        "DATA_PATH = \"...\""
      ],
      "metadata": {
        "id": "O4KUPPzNBREi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il-UOgoM3vlU",
        "outputId": "36b5b807-b4f3-4025-85b6-dfc9c1c50423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children/Models/Wav2Vec 2.0/bin\n",
        "\n",
        "!pip3 install -r /content/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children/Models/requirements.txt\n",
        "!TPU_NAME=$TPU_NAME BUCKET_NAME=$BUCKET_NAME DATA_PATH=$DATA_PATH python3 train.py\n",
        "\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate model"
      ],
      "metadata": {
        "id": "lHaVuAXfCsNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children/Models/Wav2Vec 2.0\n",
        "!wget https://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "!tar -xf test-clean.tar.gz"
      ],
      "metadata": {
        "id": "dUAw7ODT9I1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADDRESS_OF_PRETRAINED_MODEL = \"...\""
      ],
      "metadata": {
        "id": "O-BcD_CeEWzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from layers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(ADDRESS_OF_PRETRAINED_MODEL)"
      ],
      "metadata": {
        "id": "itdnFjAQDDjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_forward(speech):\n",
        "  tf_out = model(speech, training=False)\n",
        "  return tf.squeeze(tf.argmax(tf_out, axis=-1))"
      ],
      "metadata": {
        "id": "Nqysll4CEavb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "SPLIT = \"test-clean\"\n",
        "\n",
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ],
      "metadata": {
        "id": "Gdy7g8NsEhKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_sound_text_mapping():\n",
        "  flac_files = tf.io.gfile.glob(f\"LibriSpeech/{SPLIT}/*/*/*.flac\")\n",
        "  txt_files = tf.io.gfile.glob(f\"LibriSpeech/{SPLIT}/*/*/*.txt\")\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  file_ids = set(speech_samples.keys()) & set(txt_samples.keys())\n",
        "  print(f\"{len(file_ids)} files are found in LibriSpeech/{SPLIT}\")\n",
        "  samples = [(speech_samples[file_id], txt_samples[file_id]) for file_id in file_ids]\n",
        "  return samples"
      ],
      "metadata": {
        "id": "YDD1DvSpEjzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = fetch_sound_text_mapping()"
      ],
      "metadata": {
        "id": "LN-QDCXLEnnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from layers import Wav2Vec2Processor\n",
        "\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)"
      ],
      "metadata": {
        "id": "D4FGDo4JEoR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MAXLEN, LABEL_MAXLEN = 246000, 256\n",
        "DO_PADDING = False\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  label = tf.constant(label, dtype=tf.int32)[None]\n",
        "  if DO_PADDING:\n",
        "    label = label[:, :LABEL_MAXLEN]\n",
        "    padding = tf.zeros((label.shape[0], LABEL_MAXLEN - label.shape[1]), dtype=label.dtype)\n",
        "    label = tf.concat([label, padding], axis=-1)\n",
        "  return label\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  audio = processor(audio)[None]\n",
        "  if DO_PADDING:\n",
        "    audio = audio[:, :AUDIO_MAXLEN]\n",
        "    padding = tf.zeros((audio.shape[0], AUDIO_MAXLEN - audio.shape[1]), dtype=audio.dtype)\n",
        "    audio = tf.concat([audio, padding], axis=-1)\n",
        "  return audio"
      ],
      "metadata": {
        "id": "tt_vDf8_EzFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inputs_generator():\n",
        "  for speech, text in samples:\n",
        "    yield preprocess_speech(speech), preprocess_text(text)\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ],
      "metadata": {
        "id": "MJA6U3h_E0zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def infer_librispeech(dataset: tf.data.Dataset, num_batches: int = None):\n",
        "  predictions, labels = [], []\n",
        "  for batch in tqdm(dataset, total=num_batches, desc=\"LibriSpeech Inference ... \"):\n",
        "    speech, label = batch\n",
        "    tf_out = tf_forward(speech)\n",
        "    predictions.append(tokenizer.decode(tf_out.numpy().tolist(), group_tokens=True))\n",
        "    labels.append(tokenizer.decode(label.numpy().squeeze().tolist(), group_tokens=False))\n",
        "  return predictions, labels"
      ],
      "metadata": {
        "id": "jCeHMJY8E2m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels = infer_librispeech(dataset, num_batches=2618)"
      ],
      "metadata": {
        "id": "aootuIgkE3m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "wer = load_metric(\"wer\")\n",
        "wer.compute(references=labels, predictions=predictions)"
      ],
      "metadata": {
        "id": "KSDPUhRdE54b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the following code I have, you can use the `evaluation.py` in the `bin` directory."
      ],
      "metadata": {
        "id": "zatzvG6vFN0R"
      }
    }
  ]
}